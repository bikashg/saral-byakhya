{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of Neural Network: Hand-holding guide with Working Examples\n",
    "\n",
    "   # PART - 1\n",
    "\n",
    "\n",
    "   # Author: Bikash Gyawali\n",
    "   \n",
    "   # Date: 29 April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Covered : Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: A working Example: Learning to Predict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input(X) Output(Y)\n",
    " -2         -14\n",
    " -4         -28\n",
    " -8         -56\n",
    " -16        -112\n",
    " 2         14\n",
    " 4         28\n",
    " 8         56\n",
    " 16        112\n",
    " \n",
    " 32        ??\n",
    " \n",
    " We are looking for a prediction of 224. The weight needed to transform x to y is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-2, -4, -8, -16, 2, 4, 8, 16])\n",
    "Y = np.array([-14, -28, -56, -112, 14, 28, 56, 112])\n",
    "\n",
    "test_data =  np.array([-32])\n",
    "\n",
    "import tabletext\n",
    "data1 = [[\"Example Number\",\"X\",\"Y\"],\n",
    "        [\"i=1\",-2,-14],\n",
    "        [\"i=2\",-4,-28],\n",
    "        [\"i=3\",-8,-56],\n",
    "        [\"i=4\",-16,-112],\n",
    "        [\"i=5\",2,14],\n",
    "        [\"i=6\",4,28],\n",
    "        [\"i=7\",8,56],\n",
    "        [\"i=8\",16,112],\n",
    "        [\"i=9\",32,\"???\"],\n",
    "        ]\n",
    "print(tabletext.to_text(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "In general, $X$ referes to the matrix of the $x$ component for all examples and $x_i$ referes to the $x$ component of the $i^{th}$ example.\n",
    "\n",
    "Likewise for $Y$ and $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_weight = np.array(7)\n",
    "print(true_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X*true_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: How well can we do with random guesses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "random_weights = [randint(0, 15) for i in range(0,6)]\n",
    "random_weights.sort()\n",
    "random_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x,weight):\n",
    "    return x*weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_predictions = []\n",
    "for wt in random_weights:\n",
    "    r_predictions.append(get_prediction(X,wt))\n",
    "r_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(14,6))\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.scatter(X, Y, color='g', s=124)\n",
    "ax.plot(X, Y, linestyle='solid', color='g', label=\"True Weight\")\n",
    "\n",
    "\n",
    "r_pred_colors = ['c', 'm', 'y', 'r', 'b', 'k']\n",
    "for pred,col in zip(r_predictions,r_pred_colors):\n",
    "    ax.scatter(X, pred, color=col, s=124)\n",
    "    ax.plot(X, pred, linestyle=':', color=col, label=\"Random Weight=\"+str(random_weights[r_pred_colors.index(col)]))\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "    \n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Visualising Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Can we quantify the error?  -- Calculate Error and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(prediction,target):\n",
    "    error = []\n",
    "    for p,tgt in zip(prediction,target):\n",
    "        err = (tgt-p)**2       # Question: Why do we need to square and then square root ?\n",
    "        error.append(err)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_errors = []\n",
    "for pred in r_predictions:\n",
    "    r_errors.append(calculate_error(pred,Y))\n",
    "r_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=plt.figure(figsize=(14,6))\n",
    "ax2=fig2.add_axes([0,0,1,1])\n",
    "\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X, r_errors[0])))  # sorting for better line plot -- https://stackoverflow.com/a/37415568/530399\n",
    "ax2.scatter(xs, ys, color='c', s=124)\n",
    "ax2.plot(xs, ys, linestyle='solid', color='c', label=\"Random Weight=\"+str(random_weights[0]))\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X, r_errors[1])))\n",
    "ax2.scatter(xs, ys, color='m', s=124)\n",
    "ax2.plot(xs, ys, linestyle='solid', color='m', label=\"Random Weight=\"+str(random_weights[1]))\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X, r_errors[2])))\n",
    "ax2.scatter(xs, ys, color='y', s=124)\n",
    "ax2.plot(xs, ys, linestyle='solid', color='y', label=\"Random Weight=\"+str(random_weights[2]))\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X, r_errors[3])))\n",
    "ax2.scatter(xs, ys, color='r', s=124)\n",
    "ax2.plot(xs, ys, linestyle='solid', color='r', label=\"Random Weight=\"+str(random_weights[3]))\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X, r_errors[4])))\n",
    "ax2.scatter(xs, ys, color='b', s=124)\n",
    "ax2.plot(xs, ys, linestyle='solid', color='b', label=\"Random Weight=\"+str(random_weights[4]))\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X, r_errors[5])))\n",
    "ax2.scatter(xs, ys, color='k', s=124)\n",
    "ax2.plot(xs, ys, linestyle='solid', color='k', label=\"Random Weight=\"+str(random_weights[5]))\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.set_title('Error obtained for different inputs using different weights for the function Y = WX')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(prediction,target):\n",
    "    error = calculate_error(prediction,target)\n",
    "    avg_loss = sum(error) / len(error)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_losses = []\n",
    "for pred in r_predictions:\n",
    "    r_losses.append(calculate_loss(pred,Y))\n",
    "r_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[\"Example Number\",\"X\",\"Y\", \"Prediction\", \"Error\"],\n",
    "        [\"i=1\",-2,-14,-16, 4],\n",
    "        [\"i=2\",-4,-28,-32, 16],\n",
    "        [\"i=3\",-8,-56,-64, 64],\n",
    "        [\"i=4\",-16,-112,-128, 256],\n",
    "        [\"i=5\",2,14,16, 4],\n",
    "        [\"i=6\",4,28,32, 16],\n",
    "        [\"i=7\",8,56,64, 64],\n",
    "        [\"i=8\",16,112,128, 256],\n",
    "        ]\n",
    "print(\"Random Weight = 8\")\n",
    "print(tabletext.to_text(data2))\n",
    "print(\"Loss = 85.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "$Error_{i,k} = (y_i - (w_k * x_i))^2\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "  Loss_k = \\frac{1}{n}\\sum_{i=1}^{n} Error_{i,k} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Let the machine do the guessing  -- Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3=plt.figure(figsize=(14,6))\n",
    "ax3=fig3.add_axes([0,0,1,1])\n",
    "\n",
    "ax3.scatter(random_weights, r_losses, color='b', s=124)\n",
    "ax3.plot(random_weights, r_losses, linestyle='solid', color='b', label=\"Random Weights Selected\")\n",
    "\n",
    "ax3.scatter(true_weight, 0, color='g', s=124, label=\"True Weight\")\n",
    "\n",
    "\n",
    "all_weights = np.linspace(2, 15, 1000)\n",
    "all_losses = [calculate_loss(pred,Y) for pred in [get_prediction(X,wt) for wt in all_weights]]\n",
    "ax3.plot(all_weights, all_losses, linestyle='solid', color='r', label=\"All Possible weights\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "ax3.set_xlabel('Weight')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Loss when using different weights for the function Y = WX')\n",
    "\n",
    "plt.xticks(np.arange(min(random_weights), max(random_weights)+1, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = [[\"W1\",\"L1\", \"W2\",\"L2\", \"dW = W2-W1\",\"dL = L2-L1\", \"gradient = dL/dW\"],\n",
    "        [2, 2125, 3, 1360, 1, -765, -765],\n",
    "        [3, 1360, 4, 765, 1, -595, -595],\n",
    "        [4, 765, 8, 85, 4, -680, -170],\n",
    "        [8, 85, 14, 4165, 6, 4080, 680],\n",
    "        [14, 4165, 15, 5440, 1, 1275, 1275],\n",
    "        ]\n",
    "print(tabletext.to_text(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {}\n",
    "for idx in range(len(r_losses)-1):\n",
    "    w1 = random_weights[idx]\n",
    "    l1 = r_losses[idx]\n",
    "    w2 = random_weights[idx+1]\n",
    "    l2 = r_losses[idx+1]\n",
    "    gradients[w1] = (l2-l1)/(w2-w1)\n",
    "print (gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "def get_better_weight_numerical(input_weight, alpha=1):\n",
    "    out_weight = input_weight - alpha*gradients[input_weight]\n",
    "    return out_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_better_weight_numerical(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_better_weight_numerical(8, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(random_weights)-1):\n",
    "    org_wgt = random_weights[idx]\n",
    "    new_wgt = get_better_weight_numerical(org_wgt, alpha)\n",
    "    print(\"Better weight estimate for \"+str(org_wgt)+\" is \"+str(new_wgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "The derivative of the loss function, $\\frac{d}{dw_k}Loss_k$ = $\\frac{2}{n}\\sum_{i=1}^{n} ((w_k*x_i-y_i)*x_i)$. Check at https://www.derivative-calculator.net/#expr=%28y-wx%29%5E2&diffvar=w&showsteps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_fn(input_weight):\n",
    "    gradient = 0\n",
    "    for x_i,y_i in zip(X,Y):\n",
    "        gradient = gradient + (input_weight*x_i - y_i)*x_i\n",
    "    return (2.00*gradient)/len(Y)\n",
    "        \n",
    "def get_better_weight_algebraic(input_weight, alpha=1):\n",
    "    out_weight = input_weight - alpha*gradient_fn(input_weight)\n",
    "    return out_weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ML Libraries provide implentation of gradient descent of the standard loss functions. Examples include : \n",
    "\n",
    "Mean Squared Loss\n",
    "Cross Entropy Loss\n",
    "\n",
    "See https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7 to read on loss functions implemented on pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate until the loss is minimised\n",
    "start_weight = 14\n",
    "\n",
    "# start_weight = 147\n",
    "# start_weight = 147\n",
    "\n",
    "start_loss = calculate_loss(get_prediction(X,start_weight),Y)\n",
    "print(\"Start weight = \"+str(start_weight)+\", start loss = \"+str(start_loss))\n",
    "\n",
    "updated_weight = start_weight\n",
    "updated_loss = start_loss\n",
    "while updated_loss>0.5:\n",
    "    updated_weight = get_better_weight_algebraic(updated_weight, alpha)\n",
    "    updated_loss = calculate_loss(get_prediction(X,updated_weight),Y)\n",
    "    print(\"Updated weight = \"+str(updated_weight)+\", updated loss = \"+str(updated_loss))\n",
    "\n",
    "# or you could also iterate for a fixed number of epochs -- because you wouldn't know what the ideal threshold for updated_loss is! Also possible to do early stopping -- keep updating the weights (i.e. train) as long as the loss on validation data keeps on decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = updated_weight * test_data\n",
    "prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q: How about an analytical solution?\n",
    "Ans: \n",
    "    We know the derivative of our loss function.\n",
    "    The minimum of the function is where the derivative is 0.\n",
    "    But solving that equation is expensive!!\n",
    "    Hence the gradient descent technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Mini)batch gradient descent; Stochastic gradient descent; online gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_better_weight_algebraic_stochastic(input_weight, alpha=1):\n",
    "    out_weight = input_weight\n",
    "    for x_i,y_i in zip(X,Y):\n",
    "        current_gradient = 2.00*(input_weight*x_i - y_i)*x_i\n",
    "        out_weight = out_weight - alpha*current_gradient\n",
    "    return out_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_loss = calculate_loss(get_prediction(X,start_weight),Y)\n",
    "print(\"Start weight = \"+str(start_weight)+\", start loss = \"+str(start_loss))\n",
    "\n",
    "updated_weight = start_weight\n",
    "updated_loss = start_loss\n",
    "while updated_loss>0.5:\n",
    "    updated_weight = get_better_weight_algebraic_stochastic(updated_weight, alpha)\n",
    "    updated_loss = calculate_loss(get_prediction(X,updated_weight),Y)\n",
    "    print(\"Updated weight = \"+str(updated_weight)+\", updated loss = \"+str(updated_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW: \n",
    "\n",
    "Study about \"bias\". Bias is a learnable parameter just like the weight parameter that we saw. Think why would we need bias. How about regularization?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
